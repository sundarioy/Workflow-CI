name: MLflow Project CI - Stroke Prediction with Artifact Storage

# Trigger events
on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:  # Manual trigger
    inputs:
      experiment_name:
        description: 'MLflow experiment name'
        required: false
        default: 'stroke_prediction_ci_manual'
      enable_tuning:
        description: 'Enable hyperparameter tuning'
        required: false
        default: 'true'
        type: choice
        options:
        - 'true'
        - 'false'
      verbose:
        description: 'Enable verbose logging'
        required: false
        default: false
        type: boolean

# Environment variables
env:
  PYTHON_VERSION: '3.11'
  MLFLOW_TRACKING_URI: 'file:./mlruns'

jobs:
  # Job 1: Basic validation
  validate:
    runs-on: ubuntu-latest
    name: Validate MLProject Structure
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install MLflow
      run: |
        python -m pip install --upgrade pip
        pip install mlflow>=2.0.0
        
    - name: Validate MLProject structure
      run: |
        echo "ðŸ” Validating MLProject structure..."
        ls -la MLProject/
        echo "ðŸ“‹ Checking required files:"
        test -f MLProject/MLProject && echo "âœ… MLProject file exists" || exit 1
        test -f MLProject/python_env.yaml && echo "âœ… python_env.yaml exists" || exit 1
        test -f MLProject/modelling.py && echo "âœ… modelling.py exists" || exit 1
        test -d MLProject/stroke_data_preprocessing && echo "âœ… Data directory exists" || exit 1
        echo "ðŸŽ¯ MLProject structure validation passed!"

  # Job 2: Quick training without tuning (for PR and faster CI)
  train_fast:
    runs-on: ubuntu-latest
    name: Quick Model Training (No Tuning)
    needs: validate
    if: github.event_name == 'pull_request' || (github.event_name == 'workflow_dispatch' && github.event.inputs.enable_tuning == 'false')
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install dependencies
      run: |
        cd MLProject
        python -m pip install --upgrade pip
        pip install mlflow>=2.0.0
        pip install -r requirements.txt
        
    - name: Setup MLflow tracking
      run: |
        echo "ðŸŽ¯ Setting up MLflow tracking..."
        mkdir -p mlruns
        echo "MLFLOW_TRACKING_URI=file:./mlruns" >> $GITHUB_ENV
        
    - name: Run fast training (no tuning)
      run: |
        echo "ðŸš€ Starting fast model training (no hyperparameter tuning)..."
        cd MLProject
        
        EXPERIMENT_NAME="stroke_prediction_fast_${GITHUB_RUN_NUMBER}"
        echo "ðŸ“Š Training with experiment: $EXPERIMENT_NAME"
        
        mlflow run . \
          --env-manager local \
          --entry-point "fast" \
          -P experiment_name="$EXPERIMENT_NAME" \
          -P data_path="stroke_data_preprocessing"
          
    - name: Generate training summary
      run: |
        echo "ðŸ“Š Fast Training Summary" > training_summary.md
        echo "======================" >> training_summary.md
        echo "" >> training_summary.md
        echo "- **Trigger**: ${{ github.event_name }}" >> training_summary.md
        echo "- **Branch**: ${{ github.ref_name }}" >> training_summary.md
        echo "- **Commit**: ${{ github.sha }}" >> training_summary.md
        echo "- **Timestamp**: $(date)" >> training_summary.md
        echo "- **Training Mode**: Fast (No Hyperparameter Tuning)" >> training_summary.md
        echo "- **Run Number**: ${{ github.run_number }}" >> training_summary.md
        echo "" >> training_summary.md
        echo "## Training Results" >> training_summary.md
        echo "Models trained: Logistic Regression, Random Forest, XGBoost" >> training_summary.md
        echo "MLflow tracking data available in \`mlruns/\` directory" >> training_summary.md
        
    - name: Upload temporary artifacts
      uses: actions/upload-artifact@v4
      with:
        name: mlflow-fast-training-results
        path: |
          mlruns/
          training_summary.md
        retention-days: 7

  # Job 3: Full training with hyperparameter tuning (for main branch and manual triggers)
  train_with_tuning:
    runs-on: ubuntu-latest
    name: Full Model Training (With Tuning)
    needs: validate
    if: (github.event_name == 'push' && github.ref == 'refs/heads/main') || (github.event_name == 'workflow_dispatch' && github.event.inputs.enable_tuning == 'true')
    timeout-minutes: 60
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential
        
    - name: Install MLflow and dependencies
      run: |
        cd MLProject
        python -m pip install --upgrade pip
        pip install mlflow>=2.0.0
        pip install -r requirements.txt
        
    - name: Setup MLflow tracking
      run: |
        echo "ðŸŽ¯ Setting up MLflow tracking..."
        mkdir -p mlruns
        echo "MLFLOW_TRACKING_URI=file:./mlruns" >> $GITHUB_ENV
        
    - name: Run full training with hyperparameter tuning
      run: |
        echo "ðŸš€ Starting full model training with hyperparameter tuning..."
        cd MLProject
        
        if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
          EXPERIMENT_NAME="${{ github.event.inputs.experiment_name }}"
        else
          EXPERIMENT_NAME="stroke_prediction_tuned_${GITHUB_RUN_NUMBER}"
        fi
        
        echo "ðŸ“Š Training with experiment: $EXPERIMENT_NAME"
        echo "ðŸ”§ Hyperparameter tuning: ENABLED"
        
        # Choose entry point based on verbose setting
        if [ "${{ github.event.inputs.verbose }}" = "true" ]; then
          ENTRY_POINT="verbose"
        else
          ENTRY_POINT="main"
        fi
        
        mlflow run . \
          --env-manager local \
          --entry-point "$ENTRY_POINT" \
          -P experiment_name="$EXPERIMENT_NAME" \
          -P data_path="stroke_data_preprocessing"
          
    - name: Extract model performance metrics
      run: |
        echo "ðŸ“Š Extracting model performance metrics..."
        python -c "
        import mlflow
        import pandas as pd
        import json
        import os
        
        # Set tracking URI
        mlflow.set_tracking_uri('file:./mlruns')
        
        try:
            # Get all experiments
            experiments = mlflow.search_experiments()
            metrics_summary = {}
            
            for exp in experiments:
                runs = mlflow.search_runs(exp.experiment_id)
                if not runs.empty:
                    # Extract key metrics
                    metrics_summary[exp.name] = {
                        'total_runs': len(runs),
                        'best_f1_score': runs.select_dtypes(include=['number']).filter(regex='f1_score').max().max() if not runs.select_dtypes(include=['number']).filter(regex='f1_score').empty else 0,
                        'best_accuracy': runs.select_dtypes(include=['number']).filter(regex='accuracy').max().max() if not runs.select_dtypes(include=['number']).filter(regex='accuracy').empty else 0,
                        'best_roc_auc': runs.select_dtypes(include=['number']).filter(regex='roc_auc').max().max() if not runs.select_dtypes(include=['number']).filter(regex='roc_auc').empty else 0
                    }
            
            # Save metrics summary
            with open('metrics_summary.json', 'w') as f:
                json.dump(metrics_summary, f, indent=2)
            
            print('âœ… Metrics extracted successfully')
            
        except Exception as e:
            print(f'âš ï¸ Error extracting metrics: {e}')
            # Create empty summary
            with open('metrics_summary.json', 'w') as f:
                json.dump({}, f)
        "
        
    - name: Generate comprehensive training report
      run: |
        echo "ðŸ“Š Full Training Summary" > training_summary.md
        echo "======================" >> training_summary.md
        echo "" >> training_summary.md
        echo "- **Trigger**: ${{ github.event_name }}" >> training_summary.md
        echo "- **Branch**: ${{ github.ref_name }}" >> training_summary.md
        echo "- **Commit**: ${{ github.sha }}" >> training_summary.md
        echo "- **Timestamp**: $(date)" >> training_summary.md
        echo "- **Training Mode**: Full (With Hyperparameter Tuning)" >> training_summary.md
        echo "- **Run Number**: ${{ github.run_number }}" >> training_summary.md
        echo "" >> training_summary.md
        echo "## Training Configuration" >> training_summary.md
        echo "- Models: Logistic Regression, Random Forest, XGBoost" >> training_summary.md
        echo "- Hyperparameter Tuning: Enabled" >> training_summary.md
        echo "- Cross-validation: 3-fold" >> training_summary.md
        echo "- Tuning Strategy: GridSearchCV + RandomizedSearchCV" >> training_summary.md
        echo "" >> training_summary.md
        echo "## Artifacts Generated" >> training_summary.md
        echo "- MLflow tracking data (\`mlruns/\`)" >> training_summary.md
        echo "- Trained model files" >> training_summary.md
        echo "- Performance metrics (\`metrics_summary.json\`)" >> training_summary.md
        echo "- Hyperparameter tuning results" >> training_summary.md
        echo "" >> training_summary.md
        echo "## Model Performance" >> training_summary.md
        if [ -f "metrics_summary.json" ]; then
          echo "\`\`\`json" >> training_summary.md
          cat metrics_summary.json >> training_summary.md
          echo "\`\`\`" >> training_summary.md
        fi
        
    - name: Upload temporary artifacts
      uses: actions/upload-artifact@v4
      with:
        name: mlflow-tuning-results
        path: |
          mlruns/
          training_summary.md
          metrics_summary.json
        retention-days: 7

  # Job 4: Store artifacts to GitHub repository
  store_artifacts:
    runs-on: ubuntu-latest
    name: Store Artifacts to GitHub Repository
    needs: [train_fast, train_with_tuning]
    if: always() && (needs.train_fast.result == 'success' || needs.train_with_tuning.result == 'success')
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Fetch all history for all branches
        token: ${{ secrets.GITHUB_TOKEN }}
        
    - name: Configure Git
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action Bot"
        
    - name: Download training artifacts
      uses: actions/download-artifact@v4
      with:
        pattern: mlflow-*-results
        merge-multiple: true
        
    - name: Create artifacts directory structure
      run: |
        # Create unique artifact branch name
        TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
        BRANCH_NAME="artifacts/run-${GITHUB_RUN_NUMBER}-${TIMESTAMP}"
        echo "ARTIFACT_BRANCH=${BRANCH_NAME}" >> $GITHUB_ENV
        
        # Create artifact directory
        mkdir -p artifacts
        
        # Copy all training results
        if [ -d "mlruns" ]; then
          cp -r mlruns/ artifacts/
          echo "âœ… Copied MLflow runs"
        fi
        
        if [ -f "training_summary.md" ]; then
          cp training_summary.md artifacts/
          echo "âœ… Copied training summary"
        fi
        
        if [ -f "metrics_summary.json" ]; then
          cp metrics_summary.json artifacts/
          echo "âœ… Copied metrics summary"
        fi
        
        # Create artifact README
        cat > artifacts/README.md << EOF
        # Training Artifacts - Run ${{ github.run_number }}
        
        Generated on: $(date)
        Trigger: ${{ github.event_name }}
        Branch: ${{ github.ref_name }}
        Commit: ${{ github.sha }}
        
        ## Contents
        - \`mlruns/\`: MLflow tracking data
        - \`training_summary.md\`: Training process summary
        - \`metrics_summary.json\`: Model performance metrics
        
        ## Training Configuration
        - Models: Logistic Regression, Random Forest, XGBoost
        - Hyperparameter Tuning: ${{ needs.train_with_tuning.result == 'success' && 'Enabled' || 'Disabled' }}
        - Environment: Python ${{ env.PYTHON_VERSION }}
        
        ## Usage
        To use these artifacts:
        1. Download the artifacts from this branch
        2. Set MLflow tracking URI to the \`mlruns/\` directory
        3. Load models using MLflow APIs
        
        \`\`\`python
        import mlflow
        mlflow.set_tracking_uri("path/to/mlruns")
        # Load and use models
        \`\`\`
        EOF
        
        echo "ðŸ“ Artifact structure created:"
        ls -la artifacts/
        
    - name: Create and push artifact branch
      run: |
        # Create orphan branch for artifacts (no history from main)
        git checkout --orphan ${{ env.ARTIFACT_BRANCH }}
        
        # Remove all files from staging
        git rm -rf --cached .
        
        # Add only artifact files
        cp -r artifacts/* .
        
        # Stage artifact files
        git add .
        
        # Commit artifacts
        git commit -m "Add training artifacts from run ${{ github.run_number }}

        Trigger: ${{ github.event_name }}
        Branch: ${{ github.ref_name }}
        Commit: ${{ github.sha }}
        Timestamp: $(date)
        
        Training Summary:
        - Models: Logistic Regression, Random Forest, XGBoost
        - Hyperparameter Tuning: ${{ needs.train_with_tuning.result == 'success' && 'Enabled' || 'Disabled' }}
        - Run Number: ${{ github.run_number }}
        
        Artifacts:
        - MLflow tracking data
        - Model performance metrics
        - Training configuration details"
        
        # Push artifact branch
        git push origin ${{ env.ARTIFACT_BRANCH }}
        
        echo "âœ… Artifacts pushed to branch: ${{ env.ARTIFACT_BRANCH }}"
        
    - name: Create artifact summary comment (for PR)
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const branch = process.env.ARTIFACT_BRANCH;
          const runNumber = context.runNumber;
          const tuningEnabled = '${{ needs.train_with_tuning.result }}' === 'success';
          
          const comment = `## ðŸŽ¯ Model Training Results - Run #${runNumber}
          
          **Training completed successfully!** 
          
          ### ðŸ“Š Configuration
          - Models: Logistic Regression, Random Forest, XGBoost
          - Hyperparameter Tuning: ${tuningEnabled ? 'âœ… Enabled' : 'âŒ Disabled (Fast Mode)'}
          - Branch: \`${context.ref}\`
          - Commit: \`${context.sha.substring(0, 7)}\`
          
          ### ðŸ“ Artifacts Stored
          All training artifacts have been saved to branch: **\`${branch}\`**
          
          **Contents:**
          - ðŸ” MLflow tracking data
          - ðŸ¤– Trained model files  
          - ðŸ“Š Performance metrics
          - ðŸ“‹ Training summary report
          
          ### ðŸš€ Next Steps
          1. Review the training results in the artifacts branch
          2. Download models if needed for deployment
          3. Compare performance metrics with previous runs
          
          **Artifact Branch:** [\`${branch}\`](../tree/${branch})`;
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });

  # Job 5: Final summary and cleanup
  summary:
    runs-on: ubuntu-latest
    name: Pipeline Summary
    needs: [validate, train_fast, train_with_tuning, store_artifacts]
    if: always()
    
    steps:
    - name: Pipeline Status Summary
      run: |
        echo "ðŸŽ¯ MLflow Project CI Pipeline Summary"
        echo "====================================="
        echo "Validation: ${{ needs.validate.result }}"
        echo "Fast Training: ${{ needs.train_fast.result }}"
        echo "Full Training (Tuning): ${{ needs.train_with_tuning.result }}"
        echo "Artifact Storage: ${{ needs.store_artifacts.result }}"
        echo ""
        
        # Determine overall success
        VALIDATION_OK="${{ needs.validate.result == 'success' }}"
        TRAINING_OK="${{ needs.train_fast.result == 'success' || needs.train_with_tuning.result == 'success' }}"
        STORAGE_OK="${{ needs.store_artifacts.result == 'success' }}"
        
        if [ "$VALIDATION_OK" = "true" ] && [ "$TRAINING_OK" = "true" ] && [ "$STORAGE_OK" = "true" ]; then
          echo "âœ… Pipeline completed successfully!"
          echo ""
          if [ "${{ needs.train_with_tuning.result }}" = "success" ]; then
            echo "ðŸŽ‰ Full training with hyperparameter tuning completed"
          else
            echo "ðŸš€ Fast training completed (PR mode)"
          fi
          echo "ðŸ“Š All artifacts stored in GitHub repository"
          echo "ðŸ”— Check the artifacts branch for trained models and metrics"
        else
          echo "âŒ Pipeline failed. Check individual job logs:"
          echo "   - Validation: ${{ needs.validate.result }}"
          echo "   - Training: ${{ needs.train_fast.result || needs.train_with_tuning.result }}"
          echo "   - Storage: ${{ needs.store_artifacts.result }}"
          exit 1
        fi
        
    - name: Output artifact information
      if: needs.store_artifacts.result == 'success'
      run: |
        echo "ðŸ“ ARTIFACT STORAGE SUMMARY"
        echo "=========================="
        echo "âœ… Training artifacts successfully stored in GitHub repository"
        echo "ðŸ”— Artifact branch: artifacts/run-${{ github.run_number }}-*"
        echo "ðŸ“Š Contains: MLflow runs, model files, metrics, and reports"
        echo "ðŸŽ¯ Ready for model deployment and analysis"