name: MLflow Project CI - Stroke Prediction with Artifact Storage

# Trigger events
on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:  # Manual trigger
    inputs:
      experiment_name:
        description: 'MLflow experiment name'
        required: false
        default: 'stroke_prediction_ci_manual'
      enable_tuning:
        description: 'Enable hyperparameter tuning'
        required: false
        default: 'true'
        type: choice
        options:
        - 'true'
        - 'false'
      verbose:
        description: 'Enable verbose logging'
        required: false
        default: false
        type: boolean

# Environment variables
env:
  PYTHON_VERSION: '3.11'
  MLFLOW_TRACKING_URI: 'file:./mlruns'

jobs:
  # Job 1: Basic validation
  validate:
    runs-on: ubuntu-latest
    name: Validate MLProject Structure
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install MLflow
      run: |
        python -m pip install --upgrade pip
        pip install mlflow>=2.0.0
        
    - name: Validate MLProject structure
      run: |
        echo "🔍 Validating MLProject structure..."
        ls -la MLProject/
        echo "📋 Checking required files:"
        test -f MLProject/MLProject && echo "✅ MLProject file exists" || exit 1
        test -f MLProject/python_env.yaml && echo "✅ python_env.yaml exists" || exit 1
        test -f MLProject/modelling.py && echo "✅ modelling.py exists" || exit 1
        test -d MLProject/stroke_data_preprocessing && echo "✅ Data directory exists" || exit 1
        echo "🎯 MLProject structure validation passed!"

  # Job 2: Quick training without tuning (for PR and faster CI)
  train_fast:
    runs-on: ubuntu-latest
    name: Quick Model Training (No Tuning)
    needs: validate
    if: github.event_name == 'pull_request' || (github.event_name == 'workflow_dispatch' && github.event.inputs.enable_tuning == 'false')
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install dependencies
      run: |
        cd MLProject
        python -m pip install --upgrade pip
        pip install mlflow>=2.0.0
        pip install -r requirements.txt
        
    - name: Setup MLflow tracking
      run: |
        echo "🎯 Setting up MLflow tracking..."
        mkdir -p mlruns
        echo "MLFLOW_TRACKING_URI=file:./mlruns" >> $GITHUB_ENV
        
    - name: Run fast training (no tuning)
      run: |
        echo "🚀 Starting fast model training (no hyperparameter tuning)..."
        cd MLProject
        
        EXPERIMENT_NAME="stroke_prediction_fast_${GITHUB_RUN_NUMBER}"
        echo "📊 Training with experiment: $EXPERIMENT_NAME"
        
        mlflow run . \
          --env-manager local \
          --entry-point "fast" \
          -P experiment_name="$EXPERIMENT_NAME" \
          -P data_path="stroke_data_preprocessing"
          
    - name: Generate training summary
      run: |
        echo "📊 Fast Training Summary" > training_summary.md
        echo "======================" >> training_summary.md
        echo "" >> training_summary.md
        echo "- **Trigger**: ${{ github.event_name }}" >> training_summary.md
        echo "- **Branch**: ${{ github.ref_name }}" >> training_summary.md
        echo "- **Commit**: ${{ github.sha }}" >> training_summary.md
        echo "- **Timestamp**: $(date)" >> training_summary.md
        echo "- **Training Mode**: Fast (No Hyperparameter Tuning)" >> training_summary.md
        echo "- **Run Number**: ${{ github.run_number }}" >> training_summary.md
        echo "" >> training_summary.md
        echo "## Training Results" >> training_summary.md
        echo "Models trained: Logistic Regression, Random Forest, XGBoost" >> training_summary.md
        echo "MLflow tracking data available in \`mlruns/\` directory" >> training_summary.md
        
    - name: Upload temporary artifacts
      uses: actions/upload-artifact@v4
      with:
        name: mlflow-fast-training-results
        path: |
          mlruns/
          training_summary.md
        retention-days: 7

  # Job 3: Full training with hyperparameter tuning (for main branch and manual triggers)
  train_with_tuning:
    runs-on: ubuntu-latest
    name: Full Model Training (With Tuning)
    needs: validate
    if: (github.event_name == 'push' && github.ref == 'refs/heads/main') || (github.event_name == 'workflow_dispatch' && github.event.inputs.enable_tuning == 'true')
    timeout-minutes: 60
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential
        
    - name: Install MLflow and dependencies
      run: |
        cd MLProject
        python -m pip install --upgrade pip
        pip install mlflow>=2.0.0
        pip install -r requirements.txt
        
    - name: Setup MLflow tracking
      run: |
        echo "🎯 Setting up MLflow tracking..."
        mkdir -p mlruns
        echo "MLFLOW_TRACKING_URI=file:./mlruns" >> $GITHUB_ENV
        
    - name: Run full training with hyperparameter tuning
      run: |
        echo "🚀 Starting full model training with hyperparameter tuning..."
        cd MLProject
        
        if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
          EXPERIMENT_NAME="${{ github.event.inputs.experiment_name }}"
        else
          EXPERIMENT_NAME="stroke_prediction_tuned_${GITHUB_RUN_NUMBER}"
        fi
        
        echo "📊 Training with experiment: $EXPERIMENT_NAME"
        echo "🔧 Hyperparameter tuning: ENABLED"
        
        # Choose entry point based on verbose setting
        if [ "${{ github.event.inputs.verbose }}" = "true" ]; then
          ENTRY_POINT="verbose"
        else
          ENTRY_POINT="main"
        fi
        
        mlflow run . \
          --env-manager local \
          --entry-point "$ENTRY_POINT" \
          -P experiment_name="$EXPERIMENT_NAME" \
          -P data_path="stroke_data_preprocessing"
          
    - name: Extract model performance metrics
      run: |
        echo "📊 Extracting model performance metrics..."
        python -c "
        import mlflow
        import pandas as pd
        import json
        import os
        
        # Set tracking URI
        mlflow.set_tracking_uri('file:./mlruns')
        
        try:
            # Get all experiments
            experiments = mlflow.search_experiments()
            metrics_summary = {}
            
            for exp in experiments:
                runs = mlflow.search_runs(exp.experiment_id)
                if not runs.empty:
                    # Extract key metrics
                    metrics_summary[exp.name] = {
                        'total_runs': len(runs),
                        'best_f1_score': runs.select_dtypes(include=['number']).filter(regex='f1_score').max().max() if not runs.select_dtypes(include=['number']).filter(regex='f1_score').empty else 0,
                        'best_accuracy': runs.select_dtypes(include=['number']).filter(regex='accuracy').max().max() if not runs.select_dtypes(include=['number']).filter(regex='accuracy').empty else 0,
                        'best_roc_auc': runs.select_dtypes(include=['number']).filter(regex='roc_auc').max().max() if not runs.select_dtypes(include=['number']).filter(regex='roc_auc').empty else 0
                    }
            
            # Save metrics summary
            with open('metrics_summary.json', 'w') as f:
                json.dump(metrics_summary, f, indent=2)
            
            print('✅ Metrics extracted successfully')
            
        except Exception as e:
            print(f'⚠️ Error extracting metrics: {e}')
            # Create empty summary
            with open('metrics_summary.json', 'w') as f:
                json.dump({}, f)
        "
        
    - name: Generate comprehensive training report
      run: |
        echo "📊 Full Training Summary" > training_summary.md
        echo "======================" >> training_summary.md
        echo "" >> training_summary.md
        echo "- **Trigger**: ${{ github.event_name }}" >> training_summary.md
        echo "- **Branch**: ${{ github.ref_name }}" >> training_summary.md
        echo "- **Commit**: ${{ github.sha }}" >> training_summary.md
        echo "- **Timestamp**: $(date)" >> training_summary.md
        echo "- **Training Mode**: Full (With Hyperparameter Tuning)" >> training_summary.md
        echo "- **Run Number**: ${{ github.run_number }}" >> training_summary.md
        echo "" >> training_summary.md
        echo "## Training Configuration" >> training_summary.md
        echo "- Models: Logistic Regression, Random Forest, XGBoost" >> training_summary.md
        echo "- Hyperparameter Tuning: Enabled" >> training_summary.md
        echo "- Cross-validation: 3-fold" >> training_summary.md
        echo "- Tuning Strategy: GridSearchCV + RandomizedSearchCV" >> training_summary.md
        echo "" >> training_summary.md
        echo "## Artifacts Generated" >> training_summary.md
        echo "- MLflow tracking data (\`mlruns/\`)" >> training_summary.md
        echo "- Trained model files" >> training_summary.md
        echo "- Performance metrics (\`metrics_summary.json\`)" >> training_summary.md
        echo "- Hyperparameter tuning results" >> training_summary.md
        echo "" >> training_summary.md
        echo "## Model Performance" >> training_summary.md
        if [ -f "metrics_summary.json" ]; then
          echo "\`\`\`json" >> training_summary.md
          cat metrics_summary.json >> training_summary.md
          echo "\`\`\`" >> training_summary.md
        fi
        
    - name: Upload temporary artifacts
      uses: actions/upload-artifact@v4
      with:
        name: mlflow-tuning-results
        path: |
          mlruns/
          training_summary.md
          metrics_summary.json
        retention-days: 7

  # Job 4: Store artifacts to GitHub repository
  store_artifacts:
    runs-on: ubuntu-latest
    name: Store Artifacts to GitHub Repository
    needs: [train_fast, train_with_tuning]
    if: always() && (needs.train_fast.result == 'success' || needs.train_with_tuning.result == 'success')
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Fetch all history for all branches
        token: ${{ secrets.GITHUB_TOKEN }}
        
    - name: Configure Git
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action Bot"
        
    - name: Download training artifacts
      uses: actions/download-artifact@v4
      with:
        pattern: mlflow-*-results
        merge-multiple: true
        
    - name: Create artifacts directory structure
      run: |
        # Create unique artifact branch name
        TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
        BRANCH_NAME="artifacts/run-${GITHUB_RUN_NUMBER}-${TIMESTAMP}"
        echo "ARTIFACT_BRANCH=${BRANCH_NAME}" >> $GITHUB_ENV
        
        # Create artifact directory
        mkdir -p artifacts
        
        # Copy all training results
        if [ -d "mlruns" ]; then
          cp -r mlruns/ artifacts/
          echo "✅ Copied MLflow runs"
        fi
        
        if [ -f "training_summary.md" ]; then
          cp training_summary.md artifacts/
          echo "✅ Copied training summary"
        fi
        
        if [ -f "metrics_summary.json" ]; then
          cp metrics_summary.json artifacts/
          echo "✅ Copied metrics summary"
        fi
        
        # Create artifact README
        cat > artifacts/README.md << EOF
        # Training Artifacts - Run ${{ github.run_number }}
        
        Generated on: $(date)
        Trigger: ${{ github.event_name }}
        Branch: ${{ github.ref_name }}
        Commit: ${{ github.sha }}
        
        ## Contents
        - \`mlruns/\`: MLflow tracking data
        - \`training_summary.md\`: Training process summary
        - \`metrics_summary.json\`: Model performance metrics
        
        ## Training Configuration
        - Models: Logistic Regression, Random Forest, XGBoost
        - Hyperparameter Tuning: ${{ needs.train_with_tuning.result == 'success' && 'Enabled' || 'Disabled' }}
        - Environment: Python ${{ env.PYTHON_VERSION }}
        
        ## Usage
        To use these artifacts:
        1. Download the artifacts from this branch
        2. Set MLflow tracking URI to the \`mlruns/\` directory
        3. Load models using MLflow APIs
        
        \`\`\`python
        import mlflow
        mlflow.set_tracking_uri("path/to/mlruns")
        # Load and use models
        \`\`\`
        EOF
        
        echo "📁 Artifact structure created:"
        ls -la artifacts/
        
    - name: Create and push artifact branch
      run: |
        # Create orphan branch for artifacts (no history from main)
        git checkout --orphan ${{ env.ARTIFACT_BRANCH }}
        
        # Remove all files from staging
        git rm -rf --cached .
        
        # Add only artifact files
        cp -r artifacts/* .
        
        # Stage artifact files
        git add .
        
        # Commit artifacts
        git commit -m "Add training artifacts from run ${{ github.run_number }}

        Trigger: ${{ github.event_name }}
        Branch: ${{ github.ref_name }}
        Commit: ${{ github.sha }}
        Timestamp: $(date)
        
        Training Summary:
        - Models: Logistic Regression, Random Forest, XGBoost
        - Hyperparameter Tuning: ${{ needs.train_with_tuning.result == 'success' && 'Enabled' || 'Disabled' }}
        - Run Number: ${{ github.run_number }}
        
        Artifacts:
        - MLflow tracking data
        - Model performance metrics
        - Training configuration details"
        
        # Push artifact branch
        git push origin ${{ env.ARTIFACT_BRANCH }}
        
        echo "✅ Artifacts pushed to branch: ${{ env.ARTIFACT_BRANCH }}"
        
    - name: Create artifact summary comment (for PR)
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const branch = process.env.ARTIFACT_BRANCH;
          const runNumber = context.runNumber;
          const tuningEnabled = '${{ needs.train_with_tuning.result }}' === 'success';
          
          const comment = `## 🎯 Model Training Results - Run #${runNumber}
          
          **Training completed successfully!** 
          
          ### 📊 Configuration
          - Models: Logistic Regression, Random Forest, XGBoost
          - Hyperparameter Tuning: ${tuningEnabled ? '✅ Enabled' : '❌ Disabled (Fast Mode)'}
          - Branch: \`${context.ref}\`
          - Commit: \`${context.sha.substring(0, 7)}\`
          
          ### 📁 Artifacts Stored
          All training artifacts have been saved to branch: **\`${branch}\`**
          
          **Contents:**
          - 🔍 MLflow tracking data
          - 🤖 Trained model files  
          - 📊 Performance metrics
          - 📋 Training summary report
          
          ### 🚀 Next Steps
          1. Review the training results in the artifacts branch
          2. Download models if needed for deployment
          3. Compare performance metrics with previous runs
          
          **Artifact Branch:** [\`${branch}\`](../tree/${branch})`;
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });

  # Job 5: Final summary and cleanup
  summary:
    runs-on: ubuntu-latest
    name: Pipeline Summary
    needs: [validate, train_fast, train_with_tuning, store_artifacts]
    if: always()
    
    steps:
    - name: Pipeline Status Summary
      run: |
        echo "🎯 MLflow Project CI Pipeline Summary"
        echo "====================================="
        echo "Validation: ${{ needs.validate.result }}"
        echo "Fast Training: ${{ needs.train_fast.result }}"
        echo "Full Training (Tuning): ${{ needs.train_with_tuning.result }}"
        echo "Artifact Storage: ${{ needs.store_artifacts.result }}"
        echo ""
        
        # Determine overall success
        VALIDATION_OK="${{ needs.validate.result == 'success' }}"
        TRAINING_OK="${{ needs.train_fast.result == 'success' || needs.train_with_tuning.result == 'success' }}"
        STORAGE_OK="${{ needs.store_artifacts.result == 'success' }}"
        
        if [ "$VALIDATION_OK" = "true" ] && [ "$TRAINING_OK" = "true" ] && [ "$STORAGE_OK" = "true" ]; then
          echo "✅ Pipeline completed successfully!"
          echo ""
          if [ "${{ needs.train_with_tuning.result }}" = "success" ]; then
            echo "🎉 Full training with hyperparameter tuning completed"
          else
            echo "🚀 Fast training completed (PR mode)"
          fi
          echo "📊 All artifacts stored in GitHub repository"
          echo "🔗 Check the artifacts branch for trained models and metrics"
        else
          echo "❌ Pipeline failed. Check individual job logs:"
          echo "   - Validation: ${{ needs.validate.result }}"
          echo "   - Training: ${{ needs.train_fast.result || needs.train_with_tuning.result }}"
          echo "   - Storage: ${{ needs.store_artifacts.result }}"
          exit 1
        fi
        
    - name: Output artifact information
      if: needs.store_artifacts.result == 'success'
      run: |
        echo "📁 ARTIFACT STORAGE SUMMARY"
        echo "=========================="
        echo "✅ Training artifacts successfully stored in GitHub repository"
        echo "🔗 Artifact branch: artifacts/run-${{ github.run_number }}-*"
        echo "📊 Contains: MLflow runs, model files, metrics, and reports"
        echo "🎯 Ready for model deployment and analysis"